                     +-------------------------+
                     |        CS 140           |
                     | PROJECT 4: FILE SYSTEMS |
                     |     DESIGN DOCUMENT     |
                     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Evelyn Gillie egillie@stanford.edu
Peter Pham ptpham@stanford.edu
Diego Pontoriero dpontori@stanford.edu

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

We did the assignment with the VM project enabled.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

No additional sources consulted.

                     INDEXED AND EXTENSIBLE FILES
                     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

// This is the number of block slots total in a sector
#define INODE_NUM_BLOCKS 125

// This is the number of blocks that consistently exists at any level
// of indirection
#define INODE_CONSISTENT_BLOCKS 122

// These are the total sizes for each level from the perspective of
// the root inode sector
#define INODE_DIRECT_SIZE (INODE_CONSISTENT_BLOCKS*BLOCK_SECTOR_SIZE)
#define INODE_INDIRECT_SIZE (INODE_CONSISTENT_BLOCKS*BLOCK_SECTOR_SIZE)
#define INODE_DUBINDER_SIZE (INODE_INDIRECT_SIZE*INODE_CONSISTENT_BLOCKS)

// These are the number of blocks of these levels at the root level
#define INODE_NUM_INDIRECT_BLOCKS 1
#define INODE_NUM_DUBINDER_BLOCKS 2

// These are the offsets in terms of bytes into each of the following
// levels
#define INODE_INDIRECT_OFFSET INODE_DIRECT_SIZE
#define INODE_DUBINDER_OFFSET (INODE_INDIRECT_OFFSET+INODE_NUM_INDIRECT_BLOCKS*INODE_INDIRECT_SIZE)

// These are the indices of the block in the array of blocks
#define INODE_INDIRECT_INDEX_BASE INODE_CONSISTENT_BLOCKS
#define INODE_DUBINDER_INDEX_BASE (INODE_CONSISTENT_BLOCKS+INODE_NUM_INDIRECT_BLOCKS)

/* On-disk inode.
   Must be exactly BLOCK_SECTOR_SIZE bytes long. */
struct inode_disk
{
  /* All of the inode blocks contains INODE_CONSISTENT_BLOCKS of 
     blocks for the next level of indirection. The root block uses
     INODE_CONSISTENT_BLOCKS + 1 for the singly indirect block and
     INODE_CONSISTENT_BLOCKS + 2 for the doubly indirect block */
  block_sector_t sectors[INODE_NUM_BLOCKS];
  off_t length;                 /* File size in bytes. */
  bool directory;               /* true if this inode represents a directory */
  uint8_t padding[3];           /* padding */
  unsigned magic;               /* Magic number. */
};

/* In-memory inode. */
struct inode {
  block_sector_t disk_block;    /* Sector of this inode on disk*/
  struct list_elem elem;        /* Element in inode list. */
  off_t length;
  bool directory;               /* true if this inode represents a directory */
  int open_cnt;                 /* Number of openers. */
  bool removed;                 /* True if deleted, false otherwise. */
  int deny_write_cnt;           /* 0: writes ok, >0: deny writes. */
  int deny_remove_cnt;          /* 0: removes ok, >0: deny removes.*/
  struct lock lock;             
};


>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

Level 0 -- 122 sectors * 512 bytes per sector 
          = 62,464 bytes
Level 1 -- 1 indirect block * 122 sectors * 512 bytes per sector 
          = 62,464 bytes
Level 2 -- 2 doubly indirect blocks * 122 indirect blocks *
            62,464 bytes per indirect block
          = 15,241,216 bytes
Total   -- 15,366,144 bytes


---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

A query for the sector that contains a particular offset into the file
is atomic along with allocating the sector if it doesn't exist. This
means that if there are two threads trying to write to the same offset
beyond the end of the file, exactly one sector will be allocated for
it. We accomplish this simply by locking the entire byte_to_sector
function based on the inode lock. We also lock the statement that
updates the length of the inode after a write beyond the end of the
file if the logical length is smaller than the offset of the last byte
we wrote. Thus, in a race condition, the larger length will always 
persist after all threads are done.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

Since an update to the length field happens only after the entire
extension has been written, A will see no data in our implementation 
if it tries to read while B is writing. A will see the entire 
extension if it tries to read after B has finished writing.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

Our synchronization scheme provides as much fairness as the thread
scheduler because we have no condition variables at the inode level;
readers and writers are not treated differently when they are trying
to acquire a lock.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Our inode struct is a multilevel index. We chose 2 doubly indirect 
blocks because it would easily get us the needed file size 
representation. There is a single singly indirect block but this was
more or less arbitrary. It is possible to modify the number of each of
these blocks in our implementation.  In general, one would want to
have a number of indirect blocks such that the number of bytes
representable covers most of the "medium sized" files in the system.
Since we wanted to keep the one to one mapping between inodes and
sector numbers, we filled the rest of the root inode sector with
direct blocks.

                            SUBDIRECTORIES
                            ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct dir 
{
  struct inode *inode;   /* Backing store. */
  off_t pos;             /* Current position. */
  struct lock l;		 /* Directory lock */
};
This is our in-memory directory representation.  Each directory is backed
by an inode.

struct dir_entry 
{
  block_sector_t inode_sector;        /* Sector number of header. */
  char name[NAME_MAX + 1];            /* Null terminated file name. */
  bool in_use;                        /* In use or free? */
};

Each entry within a directory contains a flag indicating that it's free or
in use, and if it's in use, it contains the file or directory's sector and
name.

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

When we start to parse a path, we first identify the starting point from
which we traverse the file system.  If the first character in the path is
'/', we start from the root, and if it isn't, then we start from the
thread's current working directory.  Then, with this starting point saved
as our "traversal current directory", we iterate over the path string,
looking up every token separated by '/' in our traversal current working
directory, and updating our traversal cwd to that entry if it is found.
If we couldn't find it, we return error.  When we reach the last token in
the string, we return that directory.

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

Reads and writes to the directory are protected by a directory lock.  In
the case where two processes try to delete an entry, one will execute
dir_remove atomically and remove the entry, and the next will acquire the
lock and won't be able to find the entry it's trying to remove.  In the
case where two threads try to create a file with the same name, one will
execute atomically under the directory lock, release the lock, and the
second thread will acquire the directory lock, see that something by that
name already exists, and return without re-adding the file.  All
operations that modify a directory are run under the directory lock.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

We abstract a lot of the directory logic to the inode layer.  When we call
dir_remove, that calls inode_remove, which marks an inode for deletion,
and it will only get deleted when it is closed by all processes who have
it open.  
If we remove a process' current working directory, we don't take
any action immediately, but the next time that thread attempts an
operation using its current working directory, it won't be able to see the
removed directory entries, including '.'.  That thread can escape the bad
state by changing directories using an absolute path, however.  We chose
to model this after the Linux style of directory deletion -- a directory
can be deleted from underneath a process, but the process doesn't discover
this until it attempts an operation on the current directory.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

We chose to represent the current directory of a process as a field in
struct thread, which stores the block number of the current directory.  On
thread creation, we initialize it to the root directory's sector, or an
inherited directory's sector.  We chose the sector number over, say, the
directory's string representation because this was more robust when it
came to name changes -- the directory's name may change, but the sector
backing it shouldn't change. 

                             BUFFER CACHE
                             ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Size of buffercache -- 64 blocks */
#define BUFFERCACHE_SIZE 64

/**
 * Denotes the type of sector held in the cache block
 */
enum sector_type
{
  METADATA,                     /* Metadata sector */
  REGULAR                       /* Regular sector */
};

/**
 * Flags to denote the state of a cache block
 */
enum cache_state
{
  READING = 0,
  WRITE_REQUESTED,
  WRITING,
  CLOCK,
  READY
};

/**
 * Names various states for buffer cache blocks for use with the clock
 * algorithm.
 */
enum cache_accessed
{
  CLEAN = 0x00,                 /* Untouched */
  ACCESSED = 0x01,              /* Accessed bit */
  DIRTY = 0x02,                 /* Dirty bit */
  META = 0x04,                  /* Metadata bit */
};

/**
 * A single entry in the buffer cache
 */
struct cache_entry
{
  void *kaddr;                  /* Address of cache block */
  int accessors;                /* Number of threads accessing buffer */
  block_sector_t sector;        /* Sector of block */
  block_sector_t next_sector;   /* Sector block will contain next */
  enum cache_state state;       /* Current state of block */
  enum cache_accessed accessed;	/* Accessed bits for block */
  enum sector_type type;        /* The type of sector */
  struct condition c;           /* To notify waiting threads */
};

 /* 30 second buffercache flush frequency */
#define BUFFERCACHE_FLUSH_FREQUENCY 30 * 1000

/**
 * List entry for a sector readahead action
 */
struct readahead_entry
{
  block_sector_t sector;        /* Sector to read ahead */
  struct list_elem elem;        /* List element */
};

static struct cache_entry *cache;      /* Cache entry table */
static struct lock cache_lock;         /* Lock for entry table */
static struct condition entries_ready; /* Signal for clock algorithm if no
                                        * blocks available */
static int cache_size;                 /* Size of the cache */
static int clock_hand;                 /* For clock algorithm */
static struct list readahead_list;     /* List of readahead blocks */
static struct lock readahead_lock;     /* Protects readahead_list */
static struct condition readahead_data; /* Notifies for readahead_list */

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

Our cache replacement system implements the clock algorithm to choose a block
to evict. Each cache entry has flags for accessed and dirty bits. As with
virtual memory page tables, we reset flip the accessed bit to give entries a
second chance if they have been recently accessed.

Instead of pinning, our cache state machine has other cache entry states
(READING, WRITING, etc.) that cause entries to be ignored when those blocks are
being operated on, since reading and writing are non-blocking and so the cache
lock isn't actually held while those operations occur.

One optimization is that we added an additional bit called METADATA which
essentially gives METADATA (i.e. inode) blocks an additional chance before
being evicted, since these are ostensibly more useful to keep in memory.

>> C3: Describe your implementation of write-behind.

Write-behind is implemented by a daemon thread that is started immediately
after the buffercache is brought online. In an infinite loop it sleeps for 30
seconds before waking and invoking a flush on each of the 64 entries in the
cache. It is non-blocking, that is, if a block is currently reading or writing
the worker simply ignores it.

>> C4: Describe your implementation of read-ahead.

Read-ahead is implemented by another daemon thread that is started after the
buffercache is brought online. There is a shared linked list queue, lock, and
condition variable that are used to create a producer-consumer releationship
between the buffercache_read () and buffercache_write () methods and the
readahead worker.

Each time a read occurs the "next" block is added to the queue by the calling
thread, followed by a cond_broadcast (). The daemon thread then awakes,
shifts all of the readahead list entries to its own private list (so it doesn't
have to hold the lock on the shared list) and then performs the I/O needed to
bring each of them into the cache in turn.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

Reading and writing to the actual block data has to support concurrency, so we
do not simply lock the entry. Instead, we have a count of accessors that is
updated within a critical section that does lock the cache entry. If a block is
evicted it has to wait until this count reaches 0, which is signaled using a
condition variable.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

To implement non-blocking I/O for reading and writing we had to ensure that we
were not holding any locks that protect the cache entries while the I/O
occurred. We accomplished this by creating a state machine. Each cache entry
has a cache_state field which denotes several states, such as READY,
WRITE_REQUESTED, WRITING, READING, etc. Each of these states prevents blocks
from being evicted or brought in if they are currently being reading or writing
from/to disk.

In conjuction with the counter described in C5 that prevents this I/O from
ocurring until all threads currently accessing the block are done (and the state
machine uses non-ready states to prevent new threads from coming in, since this
could lead to starvation), if a thread requests reading or writing to a block
that is being brought into cache, it blocks and waits for the condition
variable for that entry to be signaled.

In order to make this work we needed to include one additional field: the next
sector that an entry is scheduled to hold. This was necessary to eliminate a
race when one thread requested to read a block, was waiting for another block
to be evicted before reading it in, and while the write-out occurred another
block requested to read the same block. However, this seems to work, again with
absolutely no cache locks held during any I/O.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

File workloads likely to benefit from buffer caching will read and write
repeatedly to the same blocks before closing the files. Workloads likely to
benefit from read-ahead are ones that access the data in files sequentially
(likely a common occurrence). Workloads likely to benefit from write-behind are
ones that open many files and keep them open until they close, but don't modify
all of them that frequently.

                           SURVEY QUESTIONS
                           ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It felt average. In fact, we were expecting more work.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Yes, the filesystem aspect was interesting. However, it did feel like it was a
little more tightly-coupled with the OS than one might expect (given that in
Linux, for example, one can choose between different filesystems, leading one
to believe that typically they are more modular).

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

No, the assignment was straightforward.

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

No suggestions.

>> Any other comments?

No other comments.
